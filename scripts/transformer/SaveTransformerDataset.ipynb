{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notebook to save dataset for transformer training (Chignolin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning on use of the timeseries module: If the inherent timescales of the system are long compared to those being analyzed, this statistical inefficiency may be an underestimate.  The estimate presumes the use of many statistically independent samples.  Tests should be performed to assess whether this condition is satisfied.   Be cautious in the interpretation of the data.\n",
      "\n",
      "****** PyMBAR will use 64-bit JAX! *******\n",
      "* JAX is currently set to 32-bit bitsize *\n",
      "* which is its default.                  *\n",
      "*                                        *\n",
      "* PyMBAR requires 64-bit mode and WILL   *\n",
      "* enable JAX's 64-bit mode when called.  *\n",
      "*                                        *\n",
      "* This MAY cause problems with other     *\n",
      "* Uses of JAX in the same code.          *\n",
      "******************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "import matplotlib.pyplot as plt\n",
    "from cgp import SideChainLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = np.array(os.listdir(\"./uncoupled_gmm_dataset/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_save_folder_mame = \"./ChignolinGMMTransformerDataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (5000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(root_save_folder_mame + \"train_indices.npy\"):\n",
    "    assert not os.path.exists(root_save_folder_mame + \"val_indices.npy\")\n",
    "    assert not os.path.exists(root_save_folder_mame + \"test_indices.npy\")\n",
    "\n",
    "    rand_indices = np.random.permutation(len(all_files))\n",
    "    train_indices = all_files[rand_indices[:int(0.8*len(all_files))]]\n",
    "    val_indices = all_files[rand_indices[int(0.8*len(all_files)):int(0.9*len(all_files))]]\n",
    "    test_indices = all_files[rand_indices[int(0.9*len(all_files)):]]\n",
    "\n",
    "\n",
    "    np.save(root_save_folder_mame + \"train_indices.npy\", train_indices)\n",
    "    np.save(root_save_folder_mame + \"val_indices.npy\", val_indices)\n",
    "    np.save(root_save_folder_mame + \"test_indices.npy\", test_indices)\n",
    "else:\n",
    "    train_indices = np.load(root_save_folder_mame + \"train_indices.npy\")\n",
    "    val_indices = np.load(root_save_folder_mame + \"val_indices.npy\")\n",
    "    test_indices = np.load(root_save_folder_mame + \"test_indices.npy\")\n",
    "print(train_indices.shape, val_indices.shape, test_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/rotskoff/shriramconda3/envs/cgpomm/lib/python3.11/site-packages/mdtraj/core/trajectory.py:439: UserWarning: top= kwargs ignored since this file parser does not support it\n",
      "  warnings.warn('top= kwargs ignored since this file parser does not support it')\n"
     ]
    }
   ],
   "source": [
    "prop_temp = 300.0\n",
    "dt = 0.001\n",
    "num_steps = 5\n",
    "cutoff_to_use_kt = -50\n",
    "\n",
    "\n",
    "for (dataset_name, dataset_indices) in [(\"train\", train_indices), (\"val\", val_indices), (\"test\", test_indices)]:\n",
    "\n",
    "    root_relaxed_folder_name = f\"./prop_temp_{prop_temp}_dt_{dt}_num_steps_{num_steps}/\"\n",
    "    save_folder_name = f\"{root_save_folder_mame}prop_temp_{prop_temp}_dt_{dt}_num_steps_{num_steps}_cutoff_to_use_kt_{cutoff_to_use_kt}/{dataset_name}/\"\n",
    "    os.makedirs(save_folder_name, exist_ok=True)\n",
    "\n",
    "    root_uncoupled_gmm_folder_name = \"./uncoupled_gmm_dataset/\"\n",
    "    for file in dataset_indices:\n",
    "        relaxed_folder_name = f\"{root_relaxed_folder_name}{file}/\"\n",
    "        uncoupled_gmm_folder_name = f\"{root_uncoupled_gmm_folder_name}{file}/\"\n",
    "        all_energies = []\n",
    "        all_gen_components = []\n",
    "        for batch_index in range(1):\n",
    "            try:\n",
    "                energies = np.load(\n",
    "                    f\"{relaxed_folder_name}all_relaxed_potential_energies_{batch_index}.npy\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found for {file} batch {batch_index}\")\n",
    "                continue\n",
    "            gen_components = np.load(\n",
    "                f\"{uncoupled_gmm_folder_name}uncoupled_gmm_gen_components_{batch_index}.npy\", allow_pickle=True).item()\n",
    "            gen_components[7] = np.ones_like(gen_components[1]) * 64\n",
    "            concatenated_gen_components = []\n",
    "            for res_num in range(12):\n",
    "                gen_component = gen_components[res_num]\n",
    "                concatenated_gen_components.append(gen_component[:, 0])\n",
    "            concatenated_gen_components = np.stack(\n",
    "                concatenated_gen_components, axis=-1)\n",
    "\n",
    "            assert energies.shape[0] == 10000\n",
    "            assert concatenated_gen_components.shape[0] == 10000\n",
    "            all_energies.append(energies)\n",
    "            all_gen_components.append(concatenated_gen_components)\n",
    "\n",
    "\n",
    "        if len(all_energies) == 0:\n",
    "            continue\n",
    "\n",
    "        all_energies = np.concatenate(all_energies)\n",
    "        all_gen_components = np.concatenate(all_gen_components)\n",
    "\n",
    "        all_gen_components = all_gen_components[all_energies < cutoff_to_use_kt]\n",
    "        all_energies = all_energies[all_energies < cutoff_to_use_kt]\n",
    "        assert all_energies.shape[0] == all_gen_components.shape[0]\n",
    "\n",
    "        traj = md.load(f\"{uncoupled_gmm_folder_name}uncoupled_gmm_0.h5\", frame=0)\n",
    "        side_chain_lens = SideChainLens(protein_top=traj.topology)\n",
    "        side_chain_info = side_chain_lens.get_data(traj)\n",
    "        c_info = np.stack((side_chain_info[\"phi\"][\"dihedrals\"],\n",
    "                           side_chain_info[\"psi\"][\"dihedrals\"]),\n",
    "                          axis=-1)\n",
    "\n",
    "        np.save(f\"{save_folder_name}{file}_c_info.npy\", c_info)\n",
    "        np.save(f\"{save_folder_name}{file}_energies.npy\", all_energies)\n",
    "        np.save(f\"{save_folder_name}{file}_gen_components.npy\",\n",
    "                all_gen_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Transformer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 6, 0, 3, 1, 4, 2, 4, 5, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_folder_name = \"./chignolin/fg_traj/\"\n",
    "protein_filename = f\"{traj_folder_name}/chignolin_traj.h5\"\n",
    "protein_traj = md.load(protein_filename, frame=0)\n",
    "protein_top = protein_traj.topology\n",
    "all_residues =[residue.code for residue in protein_top.residues][1:-1]\n",
    "residue_names, residue_indices = np.unique(all_residues, return_inverse=True)\n",
    "residue_indices = residue_indices.reshape(-1, 10)\n",
    "residue_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_temp = 300.0\n",
    "dt = 0.001\n",
    "num_steps = 5\n",
    "cutoff_to_use_kt = -50\n",
    "root_load_folder_name = f\"./ChignolinGMMTransformerDataset/prop_temp_{prop_temp}_dt_{dt}_num_steps_{num_steps}_cutoff_to_use_kt_{cutoff_to_use_kt}/\"\n",
    "for (dataset_name, dataset_indices) in [(\"train\", train_indices), (\"val\", val_indices), (\"test\", test_indices)]:\n",
    "\n",
    "    load_folder_name = f\"{root_load_folder_name}/{dataset_name}/\"\n",
    "    all_energies = []\n",
    "    all_gen_components = []\n",
    "    all_c_info = []\n",
    "    for file in dataset_indices:\n",
    "        try:\n",
    "            c_info = np.load(f\"{load_folder_name}{file}_c_info.npy\")\n",
    "            energies = np.load(f\"{load_folder_name}{file}_energies.npy\")\n",
    "            gen_components = np.load(\n",
    "                f\"{load_folder_name}{file}_gen_components.npy\")\n",
    "        except FileNotFoundError:\n",
    "            print(file)\n",
    "            continue\n",
    "\n",
    "        all_energies.append(energies)\n",
    "        all_gen_components.append(gen_components)\n",
    "        all_c_info.append(np.repeat(c_info, energies.shape[0], axis=0))\n",
    "\n",
    "    all_energies = np.concatenate(all_energies)\n",
    "    all_gen_components = np.concatenate(all_gen_components)\n",
    "    all_c_info = np.concatenate(all_c_info)\n",
    "\n",
    "    all_src_cont = np.stack((np.sin(all_c_info[:, :, 0]),\n",
    "                             np.cos(all_c_info[:, :, 0]),\n",
    "                             np.sin(all_c_info[:, :, 1]),\n",
    "                             np.cos(all_c_info[:, :, 1])), axis=-1)\n",
    "    all_target = np.concatenate((np.ones_like(all_gen_components[:, :1]) * 65,\n",
    "                                 all_gen_components,\n",
    "                                 np.ones_like(all_gen_components[:, :1]) * 66), axis=-1)\n",
    "\n",
    "    np.save(f\"{root_load_folder_name}{dataset_name}_all_energies.npy\", all_energies)\n",
    "    np.save(f\"{root_load_folder_name}{dataset_name}_all_gen_components.npy\",\n",
    "            all_gen_components)\n",
    "    np.save(f\"{root_load_folder_name}{dataset_name}_all_c_info.npy\", all_c_info)\n",
    "\n",
    "    np.save(f\"{root_load_folder_name}{dataset_name}_all_src_cont.npy\", all_src_cont)\n",
    "    np.save(f\"{root_load_folder_name}{dataset_name}_all_src_cat.npy\", np.repeat(residue_indices, all_src_cont.shape[0], axis=0))\n",
    "    \n",
    "    np.save(f\"{root_load_folder_name}{dataset_name}_all_target.npy\", all_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cgpomm]",
   "language": "python",
   "name": "conda-env-cgpomm-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
